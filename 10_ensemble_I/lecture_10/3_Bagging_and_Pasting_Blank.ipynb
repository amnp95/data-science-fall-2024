{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40fcb5d8",
   "metadata": {},
   "source": [
    "# Bagging and Pasting\n",
    "\n",
    "Bagging and Pasting are two methods which combat over-fitting. In Bagging and Pasting we train a model on a samples of our data selected with or without replacement (respectively) and ensemble the resulting models.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Discuss bagging and pasting\n",
    "- Demonstrate how to implement them in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4431d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a051022",
   "metadata": {},
   "source": [
    "## With or without replacement\n",
    "\n",
    "If we recall from our random forest notebook we used bagging to proved each decision tree in a random forest with its own randomly selected data set. This random selection was accomplished by randomly choosing subsets of the training set <b>with</b> replacement.\n",
    "\n",
    "Both bagging and pasting refer to this process of producing a number of randomly selected subsets of the training data which are in turn used to train the same type of model/algorithm. The key difference between bagging and pasting is whether that sampling is done <b>with</b> or <b>without</b> replacement:\n",
    "\n",
    "- <i>Bagging</i>: samples <b>with</b> replacement and\n",
    "- <i>Pasting</i>: samples <b>without</b> replacement.\n",
    "\n",
    "A way to remember which is which is to remember where the origins of the term bagging. Bagging is short for Bootstrap AGGregatING. When we want to use bagging in `sklearn` we set `bootstrap = True`. So if you want sampling with replacement you want `bootstrap = True`, so you want bagging. Conversely, if you want sampling without replacement you want `bootstrap = False`, so you want pasting.\n",
    "\n",
    "Bagging/pasting can be applied to any kind of supervised learning algorithm, but the training time of the algorithm does limit how many models you can realistically use in the ensemble.\n",
    "\n",
    "## In `sklearn`\n",
    "\n",
    "`sklearn` offers the `BaggingClasifier` to build both pasting and bagging models, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html</a>.\n",
    "\n",
    "We will demonstrate this with a SVM classifier and some synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a25f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4933)\n",
    "\n",
    "X = np.zeros((200,2))\n",
    "X[:,0] = np.random.random(200)\n",
    "X[:,1] = np.random.random(200)\n",
    "\n",
    "y = np.zeros(200)\n",
    "y[X[:,0]-X[:,1]>=0.1*np.random.standard_normal(200)] = 1\n",
    "\n",
    "## to show off our decision boundary later\n",
    "xx1, xx2 = np.meshgrid(np.arange(-.01, 1.01, .01),\n",
    "                          np.arange(-.01, 1.01, .01))\n",
    "\n",
    "X_pred = np.zeros((len(xx1.reshape(-1,1)), 2))\n",
    "X_pred[:,0] = xx1.flatten()\n",
    "X_pred[:,1] = xx2.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2851058",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "plt.scatter(X[y == 0,0], \n",
    "            X[y == 0,1],\n",
    "            c='blue',\n",
    "            label=\"0\")\n",
    "plt.scatter(X[y == 1,0], \n",
    "            X[y == 1,1],\n",
    "            c='orange',\n",
    "            marker='v',\n",
    "            label=\"1\")\n",
    "plt.plot([0,1],[0,1],'k--',label=\"Actual Decision Boundary\")\n",
    "plt.xlabel(\"$x_1$\",fontsize = 12)\n",
    "plt.ylabel(\"$x_2$\",fontsize = 12)\n",
    "plt.legend(fontsize=12, loc=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the model objects here\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.ensemble import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb7a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for our SVM.  C and gamma being large means we are likely to overfit, which is what we want for this example.  \n",
    "# This is done for illustrative purposes only, and is generally not a good choice in practice!\n",
    "C = 50\n",
    "gamma= 70\n",
    "kernel = 'rbf'\n",
    "\n",
    "# Our single SVC model.\n",
    "single_model = SVC(C = C, kernel=kernel, gamma = gamma)\n",
    "\n",
    "# This is bagging since \"bootstrap = True\".  We are sampling *with* replacement.\n",
    "\n",
    "bag = BaggingClassifier(estimator = ,\n",
    "                           bootstrap = ,\n",
    "                           n_estimators = , # we are making 200 different SVC models\n",
    "                           max_samples = )   # each model is trained on 25% of the data.\n",
    "\n",
    "\n",
    "# This is pasting since \"bootstrap = False\".  We are sampling *without* replacement.\n",
    "\n",
    "paste = BaggingClassifier(estimator = ,\n",
    "                           bootstrap = ,\n",
    "                           n_estimators = , # we are making 200 different SVC models\n",
    "                           max_samples = ) # each model is trained on 25% of the data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce4e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(16,5))\n",
    "\n",
    "s = 50\n",
    "\n",
    "## Fit lr\n",
    "single_model.fit(X,y)\n",
    "y_pred = single_model.predict(X)\n",
    "\n",
    "ax[0].set_title(\"SVM classifier: overfit\")\n",
    "\n",
    "preds = single_model.predict(X_pred)\n",
    "    \n",
    "\n",
    "\n",
    "ax[0].scatter(X_pred[preds==1,0],\n",
    "            X_pred[preds==1,1],\n",
    "            alpha=.2,\n",
    "            c='orange',\n",
    "            s=10)\n",
    "ax[0].scatter(X_pred[preds==0,0],\n",
    "            X_pred[preds==0,1],\n",
    "            alpha=.2,\n",
    "            c='lightblue',\n",
    "            s=10)\n",
    "\n",
    "ax[0].scatter(X[y==0,0], \n",
    "            X[y==0,1],\n",
    "            label='Training 0',\n",
    "            c = 'blue',\n",
    "            edgecolor='black',\n",
    "            s=s)\n",
    "ax[0].scatter(X[y==1,0], \n",
    "            X[y==1,1],\n",
    "            label='Training 1',\n",
    "            c = 'darkorange',\n",
    "            marker = 'v',\n",
    "            edgecolor='black',\n",
    "            s=s)\n",
    "ax[0].plot([0,1],[0,1],'k--')\n",
    "\n",
    "\n",
    "\n",
    "## Fit Bagged Data\n",
    "bag.fit(X,y)\n",
    "y_pred = bag.predict(X)\n",
    "\n",
    "acc = sum(y == y_pred)/len(y_pred)\n",
    "\n",
    "ax[1].set_title(\"Bagging: still overfit, but better.\", fontsize=14)\n",
    "\n",
    "preds = bag.predict(X_pred)\n",
    "    \n",
    "\n",
    "\n",
    "ax[1].scatter(X_pred[preds==1,0],\n",
    "            X_pred[preds==1,1],\n",
    "            alpha=.2,\n",
    "            c='orange',\n",
    "            s=10)\n",
    "ax[1].scatter(X_pred[preds==0,0],\n",
    "            X_pred[preds==0,1],\n",
    "            alpha=.2,\n",
    "            c='lightblue',\n",
    "            s=10)\n",
    "\n",
    "ax[1].scatter(X[y==0,0], \n",
    "            X[y==0,1],\n",
    "            label='Training 0',\n",
    "            c = 'blue',\n",
    "            edgecolor='black',\n",
    "            s=s)\n",
    "\n",
    "ax[1].scatter(X[y==1,0], \n",
    "            X[y==1,1],\n",
    "            label='Training 1',\n",
    "            c = 'darkorange',\n",
    "            marker = 'v',\n",
    "            edgecolor='black',\n",
    "            s=s)\n",
    "\n",
    "ax[1].plot([0,1],[0,1],'k--')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Fit Paste Data\n",
    "paste.fit(X,y)\n",
    "y_pred = paste.predict(X)\n",
    "\n",
    "acc = sum(y == y_pred)/len(y_pred)\n",
    "\n",
    "ax[2].set_title(\"Pasting: about the same as bagging\", fontsize=14)\n",
    "\n",
    "preds = paste.predict(X_pred)\n",
    "    \n",
    "ax[2].scatter(X_pred[preds==0,0],\n",
    "            X_pred[preds==0,1],\n",
    "            alpha=.2,\n",
    "            c='lightblue',\n",
    "            s=10)\n",
    "ax[2].scatter(X_pred[preds==1,0],\n",
    "            X_pred[preds==1,1],\n",
    "            alpha=.2,\n",
    "            c='orange',\n",
    "            s=10)\n",
    "ax[2].scatter(X[y==0,0], \n",
    "            X[y==0,1],\n",
    "            label='Training 0',\n",
    "            c = 'blue',\n",
    "            edgecolor='black',\n",
    "            s=s)\n",
    "\n",
    "ax[2].scatter(X[y==1,0], \n",
    "            X[y==1,1],\n",
    "            label='Training 1',\n",
    "            c = 'darkorange',\n",
    "            marker = 'v',\n",
    "            edgecolor='black',\n",
    "            s=s)\n",
    "\n",
    "ax[2].plot([0,1],[0,1],'k--')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d28c05",
   "metadata": {},
   "source": [
    "## Why use a bagging/pasting model?\n",
    "\n",
    "Bagging or pasting introduces bias into the model through the random selection process. This is because the extremes of the data set (which could lead to overfitting in a single model) are not typically selected in the random selection process because they are rare observations. Thinking back to our bias-variance trade-off notebook, this means that bagging or pasting could improve the performance of a single base model by reducing overfitting.\n",
    "\n",
    "We see that in the plots above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbd90fc",
   "metadata": {},
   "source": [
    "## Bagging vs. Pasting\n",
    "\n",
    "In general people tend to using bagging as a default and pasting does not get used as much.\n",
    "\n",
    "The main reason is because of sample size. In order to be effective, pasting needs a large data set. With smaller data sets the \"random\" sample tends to be the same across your estimators. If you have a very large data set it may be worth trying pasting as well as bagging and comparing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cbccf5",
   "metadata": {},
   "source": [
    "## Bagging/Pasting regressors\n",
    "\n",
    "Bagging and pasting can be implemented with regression models as well, where the prediction for a particular value of $X^*$ is the average of all the bagging base model predictions. In `sklearn` it is implemented with `BaggingRegressor` <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html</a>.\n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd1e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(216)\n",
    "X = np.linspace(0,2*np.pi, 1000).reshape(-1,1)\n",
    "y = (np.sin(X) + 0.2*np.random.normal(size = X.shape)).reshape(-1)\n",
    "\n",
    "regr = BaggingRegressor(\n",
    "    estimator = ,   # try an support vector regressor with gamma = 5\n",
    "    n_estimators= , # try 1 and 100 estimators\n",
    "    max_samples= ,  # lets use just 50 samples out of the 1000.  That is only 5% of the data for each estimator!\n",
    "    random_state= 216) # Just out here representing my neighborhood.\n",
    "regr.fit(X,y.reshape(-1))\n",
    "\n",
    "plt.scatter(X,y)\n",
    "plt.plot(X,regr.predict(X), color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4f91c2",
   "metadata": {},
   "source": [
    "### A look \"under the hood\"\n",
    "\n",
    "Let's implement our own simple bagging regressor class to see what is going on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f17c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBaggingRegressor():\n",
    "    def __init__(self, estimator, kwargs = {}, n_estimators = 10, max_samples=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.estimators = [estimator(**kwargs) for i in range(n_estimators)]\n",
    "    def fit(self, X, y):\n",
    "        for estimator in self.estimators:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1-self.max_samples)\n",
    "            estimator.fit(X_train, y_train)\n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        for estimator in self.estimators:\n",
    "            preds += estimator.predict(X)\n",
    "        preds = preds/self.n_estimators\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a160321",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = CustomBaggingRegressor(estimator = SVR, kwargs={'gamma':5} ,n_estimators=100, max_samples= 0.05)  # try 1 and 100 estimators\n",
    "regr.fit(X,y.reshape(-1))\n",
    "\n",
    "plt.scatter(X,y)\n",
    "plt.plot(X,regr.predict(X), color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb9228",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "Here is a reference on bagging/pasting.\n",
    "\n",
    "<a href = \"https://link.springer.com/content/pdf/10.1007/BF00058655.pdf\">Bagging Predictors</a> by Leo Breiman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9791be3f",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
