{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Recurrent Neural Networks\n",
    "\n",
    "The final neural network architecture we will cover is the recurrent neural network.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Discuss the kinds of problems recurrent nets are designed for,\n",
    "- Give an overview of basic RNN architectures,\n",
    "- Build a RNN to predict IMDB review sentiment.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Hidden Layer RNN\n",
    "\n",
    "Recurrent neural networks (RNN) were built to deal with sequential data. Some examples of sequential data include:\n",
    "- Time series,\n",
    "- Natural language,\n",
    "- Music\n",
    "\n",
    "Consider sequential data of the form $(x_t, y_t)$, with $x_t \\in \\mathbb{R}^p$ and $y_t \\in \\mathbb{R}^m$.\n",
    "\n",
    "A Single Hidden Layer RNN has the following form.  We will call the single hidden layer the \"state\", and give it dimension $d$.  Let $\\sigma_1$ and $\\sigma_2$ be differentiable activation functions (often a single variable function applied to each coordinate).\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "h_t = \\sigma_1 (W_{hx} x_t + W_{hh} h_{t-1} + b_h)\\\\\n",
    "y_t = \\sigma_2 (W_{yh} h_t + b_y)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where the learnable parameters of the model are\n",
    "\n",
    "* $h_{-1} \\in \\mathbb{R}^d$\n",
    "* $W_{hx}$ is a $d \\times p$ matrix \n",
    "* $W_{hh}$ is a $d \\times d$ matrix\n",
    "* $b_h$ is a vector in $\\mathbb{R}^d$\n",
    "* $W_{yh}$ is a $m \\times d$ matrix\n",
    "* $b_y$ is a vector in $\\mathbb{R}^m$\n",
    "\n",
    "The most common choice for $\\sigma_1$ is $\\tanh$ and the most common choice for $\\sigma_2$ is the identity function.\n",
    "\n",
    "RNNs can be more complicated than this, but we will stick with this example for the purposes of this introduction to the concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom implementation\n",
    "\n",
    "The following custom implementation of an RNN class might help to understand what is going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "  def __init__(self, p,d,m, sigma_1 = np.tanh, sigma_2 = None, print_h = False):\n",
    "    self.h = np.zeros((d,1))\n",
    "    self.W_hh = np.random.randn(d,d)\n",
    "    self.W_hx = np.random.randn(d,p)\n",
    "    self.W_yh = np.random.randn(m,d)\n",
    "    self.b_h = np.random.randn(d,1)\n",
    "    self.b_y = np.random.randn(m,1)\n",
    "    self.sigma_1 = sigma_1\n",
    "    self.sigma_2 = sigma_2\n",
    "    self.print_h = print_h\n",
    "  def predict(self, x):\n",
    "    # update the hidden state\n",
    "    x = x.reshape(-1,1)\n",
    "    if self.print_h:\n",
    "      print(f\"old h = {self.h}\")\n",
    "    self.h = np.dot(self.W_hh, self.h) + np.dot(self.W_hx, x) + self.b_h\n",
    "    if self.sigma_1:\n",
    "      self.h = self.sigma_1(self.h)\n",
    "    if self.print_h:\n",
    "      print(f\"new h = {self.h}\")\n",
    "    # compute the output vector\n",
    "    y = np.dot(self.W_yh, self.h) + self.b_y\n",
    "    if self.sigma_2:\n",
    "      y = self.sigma_2(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(np.random.randn(3,2), np.zeros((2,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with input dimension 2, hidden state of dimension 3, output of dimension 1.\n",
    "rnn = RNN(2,3,1, print_h=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the attributes\n",
    "import pprint\n",
    "pprint.pprint(rnn.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this a few times and discuss how the outputs are being computed.\n",
    "rnn.predict(np.array([0.3, 0.1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing which gets updated when we predict is the state vector $h$.  All of the other attributes are parameters which we would update during training but not when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(rnn.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Exponential Smoothing is an instance of RNN.\n",
    "\n",
    "Just as linear regression is the most basic instance of a FFNN, exponential smoothing is the most basic instance of an RNN.\n",
    "\n",
    "Recall the set up of double exponential smoothing:\n",
    "\n",
    "We iteratively update a hidden state consisting of level $s_t$ and slope $b_t$ for a time series $y_t$.\n",
    "\n",
    "$$\n",
    "\\hat{y}_{t} = \\left\\lbrace \\begin{array}{l c c} s_{t-1} + b_{t-1} & \\text{for} & 1<t\\leq n \\\\\n",
    "                                                s_n + (t-n)b_{n}& \\text{for} & t > n \\end{array}\\right\\rbrace, \n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "s_{t} = \\alpha y_t + (1-\\alpha) (s_{t-1} + b_{t-1}), \\ s_1 = y_1,\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_{t} = \\beta (s_t - s_{t-1}) + (1-\\beta) b_{t-1}, \\ b_1 = y_2 - y_1 \\text{ and}\n",
    "$$\n",
    "\n",
    "This can be re-written (with some algebra) using our notation above as\n",
    "\n",
    "$$\n",
    "h_t = \\begin{bmatrix} \\alpha \\\\ \\alpha \\beta \\end{bmatrix} y_t + \\begin{bmatrix} 1-\\alpha & 1-\\alpha \\\\ -\\alpha\\beta & 1-\\alpha\\beta \\end{bmatrix} \\begin{bmatrix} s_{t-1} \\\\ b_{t-1}\\end{bmatrix}\\\\\n",
    "\\hat{y}_t = \\begin{bmatrix} 1 & 1\\end{bmatrix} \\begin{bmatrix} s_{t-1} \\\\ b_{t-1}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So double exponential smoothing is in fact a special case of `RNN(1,2,1)`.\n",
    "\n",
    "Let's implement it as a subclass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleExpSmooth(RNN):\n",
    "    def __init__(self, alpha, beta, print_h = False):\n",
    "        super().__init__(1,2,1, None),\n",
    "        self.W_hh = np.array([[1-alpha, 1-alpha],[-alpha*beta, 1-alpha*beta]])\n",
    "        self.W_hx = np.array([[alpha],[beta*alpha]])\n",
    "        self.W_yh = np.array([[1, 1]])\n",
    "        self.b_h = np.array([[0],[0]])\n",
    "        self.b_y = np.array([[0]])\n",
    "        self.print_h = print_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how it handles predicting a noisy linear sequence\n",
    "\n",
    "des = DoubleExpSmooth(0.7,0.8, print_h = True)\n",
    "ys = 0.8*np.random.randn(10) + 2* np.arange(10) + 1\n",
    "preds = list()\n",
    "\n",
    "for i in range(10):\n",
    "    pred = des.predict(ys[i])[0,0]\n",
    "    print(f\"\\ninput = {ys[i]:.4f}, pred = {pred:4f} \\n\")\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"RNN implementation of Double Exponential Smoothing\")\n",
    "plt.scatter(range(10),ys, label = 'Data')\n",
    "plt.scatter(range(1,11),preds, label = 'One Step Predictions', marker='x')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an RNN involves [backpropagation through time](https://en.wikipedia.org/wiki/Backpropagation_through_time).  We \"unroll\" the RNN for $k$ time steps to treat it like a feed forward neural network with $k$ layers, but where the parameters of each layer are shared.  We would then implement gradient descent on the loss function as usual.  Note that the resulting trained model will still have an \"infinite memory\" even though we only accounted for $k$ time steps in the loss minimization process.\n",
    "\n",
    "A major technical issue when training RNNs is the [vanishing/exploding gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem):  the recurrent nature of the network can lead to exponential growth/decay of the model parameters during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: IMDB sentiment analysis\n",
    "\n",
    "As an illustrative example we will use `keras` to build a sentiment classifier using IMDB movie reviews. Let's first load this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The data is stored in here\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will determine the number of vocab words in our\n",
    "## dictionary\n",
    "max_features = 10000\n",
    "\n",
    "## num_words tells keras to return the reviews so they contain only\n",
    "## the num_words most used words across all the reviews\n",
    "(X_train, y_train), (X_test,y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "## Note you may receive a warning, this is not your fault, and is due to how\n",
    "## keras is loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at the first training observation\n",
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored as a list of indices, each of which is representative of a word. Let's see what this particular review looks like, once we have translated it from indices to words. Do not focus on the following code for now, as it is not important for building the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key,value) in word_index.items()])\n",
    "\n",
    "## The first training review, where words outside the top 1000 are replaced with\n",
    "## ? marks\n",
    "print(\" \".join([reverse_word_index.get(i-3, '?') for i in X_train[0]]))\n",
    "print()\n",
    "print(\"sentiment value =\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The review above had a $y$ value of $1$, meaning that it has positive sentiment. A value of $0$ indicates a negative sentiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making our validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_train,X_val,y_train_train,y_val = train_test_split(X_train, y_train,\n",
    "                                                           test_size=.2,\n",
    "                                                           shuffle=True,\n",
    "                                                           stratify = y_train,\n",
    "                                                           random_state=440)\n",
    "                                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A huge feature of the RNN model is the ability to handle variable length inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train[0]),len(X_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow has a \"ragged\" tensor type which allows for variable lengths in some dimensions.\n",
    "We now convert our data into tensors of the appropriate type and shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import convert_to_tensor\n",
    "from tensorflow import ragged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ragged.constant(X_train)\n",
    "X_train_train = ragged.constant(X_train_train)\n",
    "X_val = ragged.constant(X_val)\n",
    "X_test = ragged.constant(X_test)\n",
    "y_train = convert_to_tensor(y_train)\n",
    "y_train_train = convert_to_tensor(y_train_train)\n",
    "y_val = convert_to_tensor(y_val)\n",
    "y_test = convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a few and see that they have different lengths.\n",
    "X_train_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Network\n",
    "\n",
    "This network will introduce two new layer types `Embedding` and `SimpleRNN`. \n",
    "\n",
    "The [`Embedding`](https://keras.io/api/layers/core_layers/embedding/) layer is preprocessing step which is specific to NLP tasks. You can think of it as a dense layer which takes your one-hot encoded vocabulary to a latent space of specified dimension.  It is implemented as its own custom class instead of just using a dense layer for a couple of reasons:  \n",
    "\n",
    "* Each input of the mode will use only a small fraction of the vocabulary, so it is inefficient to actually use one-hot encoding.\n",
    "* Creating these kinds of \"word embeddings\" is common enough, and has accumulated enough task specific techniques, that it makes sense to have a dedicated class.\n",
    "\n",
    "The [`SimpleRNN`](https://keras.io/api/layers/recurrent_layers/simple_rnn/) layer is the akin to the RNN architecture we described above, but with $y_t = h_t$, i.e.\n",
    "\n",
    "* $\\sigma_2$ is the identity function\n",
    "* $W_yh$ is the identity matrix\n",
    "* $b_y = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the keras stuff we'll need\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sequential model using\n",
    "# Embedding layer of shape (max_features,32),\n",
    "# SimpleRNN layer with hidden layer size 10 and return_sequences = False, \n",
    "# Dense layer with sigmoid activation.\n",
    "\n",
    "model = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Embedding layer has $10000 \\times 32 = 320000$ parameters \n",
    "* The SimpleRNN layer has:\n",
    "    * $W_{hx}$ gives us $10 \\times 32 = 320$ parameters\n",
    "    * $W_{hh}$ gives us $10 \\times 10 = 100$ parameters\n",
    "    * $b_h$ gives us $10$ parameters\n",
    "    * Total of $430$ parameters\n",
    "* The dense layer has $10 + 1 = 11$ parameters ($10$ weights and $1$ bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight in model.get_weights():\n",
    "    print(weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This taakes about 3 minutes to run on my computer.\n",
    "\n",
    "epochs = 8\n",
    "\n",
    "history = model.fit(X_train_train, y_train_train,\n",
    "                    epochs = epochs,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the training and validation accuracy\n",
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.scatter(range(1,epochs+1), history_dict['accuracy'], label = \"Training Accuracy\")\n",
    "plt.scatter(range(1,epochs+1), history_dict['val_accuracy'], label = \"Validation Set Accuracy\")\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the training and validation loss\n",
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.scatter(range(1,epochs+1), history_dict['loss'], label = \"Training Loss\")\n",
    "plt.scatter(range(1,epochs+1), history_dict['val_loss'], label = \"Validation Set Loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss Function Value\", fontsize=12)\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and save final model.\n",
    "\n",
    "Four epochs seems like enough.  Let's retrain on the training + validation set, see the test set accuracy, and save the model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = models.Sequential([\n",
    "    layers.Embedding(max_features, 32), \n",
    "    layers.SimpleRNN(10, return_sequences=False),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "final_model.compile(optimizer='rmsprop',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "final_model.fit(X_train, y_train,\n",
    "                epochs = epochs,\n",
    "                batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.reshape(-1).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, preds.reshape(-1).round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model.save('lecture_12_assets/imdb_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "`SimpleRNN` is actually a seldom used RNN layer type because there are more complicated `keras` layers that tend to outperform `SimpleRNN`.  In particular, the most commonly used RNN layer used in practice are [Long Short Term Memory (`LSTM`)](https://en.wikipedia.org/wiki/Long_short-term_memory) layers, which partially address the vanishing gradient problem by allowing the RNN to \"forget\" irrelevant pieces of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.  Modified by Steven Gubkin 2024.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
