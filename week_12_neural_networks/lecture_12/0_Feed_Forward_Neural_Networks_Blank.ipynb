{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Networks\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Introduce Feed Forward Neural Networks\n",
    "- Demonstrate how to implement them using `keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "set_style(\"whitegrid\")\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward network architecture\n",
    "\n",
    "Recall our parametric supervised learning framework: \n",
    "* We have *data* of the form $D = {(x_i, y_i)}$ with $x_i \\in \\mathbb{R}^p$ and $y_i \\in \\mathbb{R}^m$. \n",
    "* We have a collection of *hypothesized relationships* $f_\\theta : \\mathbb{R}^p \\to \\mathbb{R}^m$ depending on some *parameters* $\\theta \\in \\Theta$.\n",
    "* We have a *loss function* $\\ell$ which takes as input a dataset $D$ and hypothesized function $f_\\theta$ and returns a positive real number.\n",
    "* We *fit* a model by finding the parameters which minimize the loss (at least approximately, if not exactly).\n",
    "\n",
    "Feed Forward Neural Networks fit into this framework.  Our collection of hypothesized relationships have the following special form:\n",
    "\n",
    "$$\n",
    "f_\\theta: \\mathbb{R}^p \\stackrel{A_1}{\\longrightarrow} \\mathbb{R}^{h_1} \\stackrel{\\sigma_1}{\\longrightarrow} \\mathbb{R}^{h_1} \\stackrel{A_2}{\\longrightarrow} \\mathbb{R}^{h_2} \\stackrel{\\sigma_2} {\\longrightarrow} \\mathbb{R}^{h_2} \\stackrel{A_3}{\\longrightarrow} \\dots \\mathbb{R}^{h_k} \\stackrel{A_k}{\\longrightarrow} \\mathbb{R}^{h_k} \\stackrel{\\sigma_k} {\\longrightarrow} \\mathbb{R}^{m}\n",
    "$$\n",
    "\n",
    "Here each $A_i$ is an affine function of the form $A_i(\\vec{x}) = W_i\\vec{x} + b_i$ and each $\\sigma_i$ is a non-linear function.  The most common choice for $\\sigma_i$ is $\\operatorname{ReLU}(x) = \\max(0,x)$ (which is applied to each coordinate independently).\n",
    "\n",
    "Note that if we *do* take all $\\sigma_i = \\operatorname{ReLU}$, then $f_\\theta$ is a piecewise linear function.  This gives us an extremely flexible hypothesis class:  in fact one can prove that just taking functions of the form $f: \\mathbb{R}^p \\to \\mathbb{R}^m$ of the form $f =  A_2 \\circ \\operatorname{ReLU} \\circ A_1$ is sufficiently flexible to allow us to uniformly approximate **any** continuous function on a compact domain (given a large enough intermediate dimension $h$).  Results of this nature go by the name of [universal approximation theorems](https://en.wikipedia.org/wiki/Universal_approximation_theorem).\n",
    "\n",
    "It is common to visualize neural networks with pictures like the following:\n",
    "\n",
    "<img src=\"lecture_12_assets/network1.png\" width=\"60%\"></img>\n",
    "\n",
    "The way to interpret this is that each circle represents a coordinate, each arrow represents an entry of a matrix $W_i$, and the activation functions and biases are omitted from the picture.\n",
    "\n",
    "For instance the function $f:\\mathbb{R}^3 \\to \\mathbb{R}^2$ given by $f(\\vec{x}) = \\operatorname{ReLU}(W\\vec{x})$ with $W = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6\\end{bmatrix}$ could be visualized with the following picture:\n",
    "\n",
    "<img src=\"lecture_12_assets/network2.png\" width=\"30%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f_\\theta: \\mathbb{R}^p \\stackrel{A_1}{\\longrightarrow} \\mathbb{R}^{h_1} \\stackrel{\\sigma_1}{\\longrightarrow} \\mathbb{R}^{h_1} \\stackrel{A_2}{\\longrightarrow} \\mathbb{R}^{h_2} \\stackrel{\\sigma_2} {\\longrightarrow} \\mathbb{R}^{h_2} \\stackrel{A_3}{\\longrightarrow} \\dots \\mathbb{R}^{h_k} \\stackrel{A_k}{\\longrightarrow} \\mathbb{R}^{h_k} \\stackrel{\\sigma_k} {\\longrightarrow} \\mathbb{R}^{m}\n",
    "$$\n",
    "\n",
    "Some neural network specific terminology:\n",
    "\n",
    "Each $\\mathbb{R}^{d}$ in the chain is called a **layer**.  Sometimes the map \"$\\sigma_i \\circ A_i$\" is called the \"layer\" instead.  \n",
    "* The first $\\mathbb{R}^p$ is called the **input layer**.\n",
    "* The last $\\mathbb{R}^m$ is called the **output layer**.\n",
    "* Intermediate $\\mathbb{R}^{h_i}$ are called **hidden layers**.\n",
    "* Each coordinate of each $\\mathbb{R}^d$ is called a **neuron**.\n",
    "* The $\\sigma_i$ are called **activation functions**.  \n",
    "    * The analogy is to a neuron firing or not depending on the strength of a stimulus.\n",
    "* Each individual entry of the matrix $W_i$ is called a **weight**.\n",
    "* Each individual entry of the vector $b_i$ is called a **bias**.\n",
    "* Taken together the weights and biases are the **parameters** of the model.\n",
    "* The full specification of allowable $f_\\theta$ is called the **model architecture**.\n",
    "    * For now this just looks like specifying the dimensions and activation functions, but we will soon see that we can place additional restrictions on the $A_i$\n",
    "    * One example is specifying that the weight matrix $W_i$ must have a particular constrained form.\n",
    "    * If there are no special constraints on $A_i$ it is called a **linear layer**, **fully connected layer**, or **dense layer**.\n",
    "* Using a numerical algorithm to adjust the parameters to decrease the loss function is called **training** the neural network.\n",
    "    * We will most frequently use some variant of gradient descent.\n",
    "* A trained model (i.e. an architecture together with specific values for the weights and biases) is called a **checkpoint**.\n",
    "\n",
    "Note that a fully connected feed forward neural network will have $(p+1)h_1 + (h_1 +1)h2 + \\dots + (h_i + 1)h_{i+1} + \\dots + (h_k + 1)m$ parameters!  This can really add up quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A naive implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement our own version of a feed forward ReLU network to better understand the structure.  \n",
    "\n",
    "We only need to specify a list of dimensions for each layer and (optionally) a final activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUNN():\n",
    "    '''A FFNN whose layers have dimensions [p,h_1,h_2,..., h_k, m] and uses RELU activation in all but the final layer'''\n",
    "    def __init__(self,dims,sigma = None):\n",
    "        self.dims = dims\n",
    "        self.l = len(dims) - 1 # number of functions between layers\n",
    "        self.weights = [np.random.randn(dims[i+1],dims[i]) for i in range(self.l)] # randomly initialize weights\n",
    "        self.biases = [np.random.randn(dims[i+1], 1) for i in range(self.l)] # making bias shape (-1,1) so addition with a matrix broadcasts correctly.\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    @staticmethod\n",
    "    def _relu(t):\n",
    "        '''Takes input numpy array and sets negative entries to 0.0'''\n",
    "        t[t<0] = 0.0\n",
    "        return t\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''Takes matrix of features X as input, applies layers, then applies final activation function sigma'''\n",
    "        X = X.astype(float)\n",
    "        X = X.transpose() # rows of design matrix are inputs of predict\n",
    "        for i in range(self.l):\n",
    "            X = np.dot(self.weights[i], X) + self.biases[i]\n",
    "            if i != self.l-1:\n",
    "                X = self._relu(X)\n",
    "        if self.sigma:\n",
    "            X = self.sigma(X)\n",
    "        return X.transpose() # rows of output matrix are the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = ReLUNN(dims = [2,3,1], sigma= None)\n",
    "pprint.pprint(nn.__dict__)\n",
    "X = np.array([[.1,.2],[.3,.4],[.5,.6]])\n",
    "print(f\"f(X) = {nn.predict(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights and biases would be adjusted during training to minimize the loss function.  After this is done we would predict as above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have already seen some Neural Networks!\n",
    "\n",
    "Several of the machine learning techniques we have covered can be seen as simple neural networks.  For example:\n",
    "\n",
    "* Linear Regression\n",
    "    * We have a set of hypothesized relationships of the form $\\mathbb{R^p} \\stackrel{A_1}{\\longrightarrow} \\mathbb{R}$.\n",
    "    * Our loss function is Mean Squared Error.\n",
    "    * In neural network terminology we are training a single layer network with no activation function.\n",
    "* Logistic Regression\n",
    "    * We have a set of hypothesizes relationships of the form $\\mathbb{R^p} \\stackrel{A_1}{\\longrightarrow} \\mathbb{R} \\stackrel{\\sigma}{\\longrightarrow} \\mathbb{R}$ where $\\sigma$ is the sigmoid activation function.\n",
    "    * Our loss function is binary cross-entropy.\n",
    "* Multiclass Logistic Regression\n",
    "    * We have a set of hypothesizes relationships of the form $\\mathbb{R^p} \\stackrel{A_1}{\\longrightarrow} \\mathbb{R^m} \\stackrel{\\operatorname{SoftMax}}{\\longrightarrow} \\mathbb{R}^m$ where $\\operatorname{SoftMax}(\\vec{a})$ has $i^{th}$ coordinate $\\frac{\\exp(a_i)}{\\sum \\exp(a_j)}$.\n",
    "    * We use categorical cross-entropy as our loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Neural Network:  Gradient Descent\n",
    "\n",
    "In many of the machine learning algorithms we have covered so far, minimizing the loss function is relatively simple:\n",
    "\n",
    "* In linear regression we have an explicit solution of the parameters which minimize the MSE.\n",
    "* In logistic regression and SVM we don't have an explicit solutions, but we *do* have simple explicit formulas for the gradient and Hessian of the loss function which makes numerical optimization very fast.\n",
    "* Many of the other techniques we have covered also have convex loss functions and/or have optimization algorithms which make training relatively quick and inexpensive.\n",
    "\n",
    "Neural Networks are different.  The loss function, thought of as a function of the parameters, is often **highly** non-convex.  You should expect tons of saddle points, non-optimal local minima, and other terrible obstructions to numerical optimization.  Despite this (and somewhat mysteriously I might add!) neural networks trained using good old fashioned Gradient Descent are capable of amazing feats like writing poetry, identifying what song is playing, and driving cars!  Here is a brief summary of how Gradient Descent works:\n",
    "\n",
    "* Choose a **learning rate** $\\textrm{lr}$.\n",
    "* Initialize your parameters $\\theta$ randomly as $\\theta_0$.\n",
    "* At each stage $j$:\n",
    "    * Compute the gradient of the loss function $\\nabla \\ell \\big|_{\\theta_j}$.\n",
    "    * Set $\\theta_{j+1} = \\theta_j - \\textrm{lr} \\nabla \\ell \\big|_{\\theta_j}$\n",
    "* Stop when you reach some stopping condition (often when validation loss stops decreasing).\n",
    "\n",
    "In the following picture we are showing the contour lines of a loss function of two parameters.  The gradient points in the direction of steepest ascent (\"straight up the hill\") while the negative gradient points in the direction of steepest descent (\"straight down the hill\").  The gradient descent algorithm just has us start at a random point and take one little step at a time in the direction which carries us down the fastest. This will generally lead us to a local (but not necessarily global) minimum of the loss function.\n",
    "\n",
    "<img src=\"lecture_12_assets/contour.png\" width=\"40%\"></img>\n",
    "\n",
    "The gradient is computed using the chain rule, which in this context is known as **backpropagation**.  Technically backpropagation is not just the chain rule, but is a specific numerical implementation of the chain rule which avoids the inefficiencies of a naive implementation.  You can think of it as roughly [chain rule](https://en.wikipedia.org/wiki/Chain_rule) + [memoization](https://en.wikipedia.org/wiki/Memoization) = [backpropagation](https://en.wikipedia.org/wiki/Backpropagation).  Note:  a previous Erdős Alumni said this to me (Steven Gubkin) in conversation and I believe they may have coined the phrase.  I do not recall who that was though!\n",
    "\n",
    "#### Common adjustments to the gradient descent\n",
    "\n",
    "Common adjustments to the gradient descent step:\n",
    "\n",
    "1. Calculating the gradient of your loss function using a subset of your data rather than all of the data.  This is called **batch gradient descent**.  In the case that your batch size is one this is called **stochastic gradient descent**.  Getting through all of your data is called one **epoch**.\n",
    "\n",
    "2. Adjusting your learning rate during training (aka adopting a **learning rate schedule**).  There are numerous methods.\n",
    "\n",
    "3. [This wikipedia page](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) contains an exposition of many different varients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training simple feed forward neural networks with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from tensorflow import convert_to_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A synthetic regression example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will show how to approximate $\\sin$ on the interval $[1,4]$ using a function of the form\n",
    "\n",
    "$$\n",
    "[1,4] \\stackrel{A_1}{\\longrightarrow} \\mathbb{R}^9 \\stackrel{\\operatorname{ReLU}}{\\longrightarrow} \\stackrel{A_2}{\\longrightarrow} \\mathbb{R}^1\n",
    "$$\n",
    "\n",
    "by using a version of gradient descent called \"RMSProp\" to minimize the mean squared error loss between model outputs and true outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = convert_to_tensor(np.linspace(1, 4, 1000))\n",
    "y = convert_to_tensor(np.sin(X)+ 0.05*np.random.randn(1000))\n",
    "\n",
    "# Create a Sequential model.  Specify input layer of dimension 1, BatchNormalization(axis = -1), a Dense layer of dimension 9, and a Dense layer of dimension 1.\n",
    "model = \n",
    "\n",
    "# Compile model using \"rmsprop\" as the optimizer, and \"mse\" as the loss.\n",
    "\n",
    "# Fit the model for 100 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X).reshape(-1)\n",
    "\n",
    "plt.scatter(X,y, alpha = 0.6)\n",
    "plt.plot(X, preds, 'k')\n",
    "# plotting points where the slope changes\n",
    "for i in range(1,len(preds)-1):\n",
    "    if np.abs((preds[i] - preds[i-1]) - (preds[i+1] - preds[i])) > 0.00001:\n",
    "        plt.plot(X[i], preds[i], 'ko')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our approximation is piecewise linear! We can extract the weights and biases as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use just these weights and biases to recover the picture we drew above?  \n",
    "\n",
    "We could, but the batch normalization layer makes things a bit more complicated. \n",
    "\n",
    "Let's rerun it with the first two lines line commented out.  Note that I specified the input shape in the first Dense layer this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    #layers.InputLayer(1),  \n",
    "    #layers.BatchNormalization(axis = -1), # equivalent of StandardScaler().  Puts inputs on \"same scale\" as initial parameters, which helps with training.\n",
    "    layers.Dense(9, activation = 'relu', input_shape=(1,)), # R^1 --> R^9 with ReLU activation\n",
    "    layers.Dense(1) # back down to R^1 again with a pure linear layer\n",
    "])\n",
    "\n",
    "model.compile(optimizer = \"rmsprop\", # version of gradient descent which adjusts learning rate dynamically\n",
    "              loss = \"mse\", # Mean Squared Error loss function\n",
    "              metrics = [\"mean_squared_error\"])\n",
    "\n",
    "model.fit(X, y, epochs = 200) # we do 200 steps of the gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting weights in the form of our custom class\n",
    "\n",
    "W_1 = model.weights[0].numpy().transpose()\n",
    "b_1 = model.weights[1].numpy().reshape(-1,1)\n",
    "W_2 = model.weights[2].numpy().transpose()\n",
    "b_2 = model.weights[3].numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_1.shape, b_1.shape, W_2.shape, b_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    x[x<0] = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = X.numpy().reshape(1,-1)\n",
    "y_np = b_2 + np.dot(\n",
    "    W_2,\n",
    "    relu(\n",
    "        np.dot(W_1, inputs) + b_1\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,y, alpha = 0.6)\n",
    "plt.plot(X, y_np.reshape(-1), \"k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can make sure that our `RelUNN` class functions properly by plugging these learned weights in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = ReLUNN(dims = [1,9,1])\n",
    "nn.weights = [W_1,W_2]\n",
    "nn.biases = [b_1, b_2]\n",
    "nn_preds = nn.predict(X.numpy().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,y, alpha = 0.6)\n",
    "plt.plot(X, nn_preds, \"k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A regression example with real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same concrete data from problem set 11.\n",
    "\n",
    "Yeh,I-Cheng. (2007). Concrete Compressive Strength. UCI Machine Learning Repository. https://doi.org/10.24432/C5PK67."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete = pd.read_csv(\"../../data/concrete.csv\") \n",
    "concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = concrete.columns[:-1]\n",
    "target = concrete.columns[-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(concrete[features],concrete[target], test_size=0.2, random_state=216)\n",
    "\n",
    "X_tt,X_val, y_tt, y_val = train_test_split(X_train,y_train, test_size=0.2, random_state=216)\n",
    "\n",
    "X_train_tensor = convert_to_tensor(X_train)\n",
    "y_train_tensor = convert_to_tensor(y_train)\n",
    "X_tt_tensor = convert_to_tensor(X_tt)\n",
    "X_val_tensor = convert_to_tensor(X_val)\n",
    "X_test_tensor = convert_to_tensor(X_test)\n",
    "y_tt_tensor = convert_to_tensor(y_tt)\n",
    "y_val_tensor = convert_to_tensor(y_val)\n",
    "y_test_tensor = convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showcasing a learning rate scheduler\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    0.01, #initial learning rate\n",
    "    decay_steps= 1000 , # After 1000 steps we start to decrease the learning rate\n",
    "    decay_rate=0.95, # multiply learning rate by 0.95 for each epoch after\n",
    "    staircase=True)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.InputLayer(8),\n",
    "    layers.BatchNormalization(axis = -1),\n",
    "    layers.Dense(20, activation = 'relu'),\n",
    "    layers.Dense(20, activation = 'relu'),\n",
    "    layers.Dense(1,kernel_regularizer=keras.regularizers.L2(0.01)) # Showing how to regularize weights\n",
    "])\n",
    "\n",
    "model.compile(keras.optimizers.legacy.Adam(learning_rate=lr_schedule), loss = \"mse\", metrics= [keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "history = model.fit(X_tt_tensor, y_tt_tensor, epochs = 200, validation_data = (X_val_tensor, y_val_tensor), batch_size = 32) # we do 200 steps of the gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a really simple fully connected NN with two hidden layers. It gives us pretty similar performance to RandomForestRegressor without doing any hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(X_tt,y_tt)\n",
    "np.sqrt(mean_squared_error(rfr.predict(X_val),y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A classificiation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0,4*np.pi, 500)\n",
    "X_0 = np.stack([t*np.cos(t), t*np.sin(t)], axis = 0).transpose() + 0.5*np.random.standard_normal((500,2))\n",
    "X_1 = np.stack([(t+1.2)*np.cos(t), (t+1.2)*np.sin(t)], axis = 0).transpose() + 0.5*np.random.standard_normal((500,2))\n",
    "X = np.concatenate([X_0,X_1], axis = 0)\n",
    "y = np.concatenate([np.zeros(500), np.ones(500)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_0[:,0],X_0[:,1])\n",
    "plt.scatter(X_1[:,0],X_1[:,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = keras.Sequential([\n",
    "    layers.Normalization(axis=-1), # Basically StandardScaler()\n",
    "    layers.Dense(50, activation = 'relu', use_bias = True ,input_shape=(2,)), # R^2 --> R^12 with ReLU activation\n",
    "    layers.Dense(50, activation = 'relu', use_bias = True), # R^12 --> R^12 with ReLU activation\n",
    "    layers.Dense(1, use_bias = True, activation = 'sigmoid') # R^12 --> R^1 again with sigmoid activation\n",
    "])\n",
    "\n",
    "clf.compile(optimizer = \"rmsprop\", \n",
    "              loss = \"binary_crossentropy\", \n",
    "              metrics = [\"accuracy\"])\n",
    "\n",
    "clf.fit(X, y, epochs = 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.mgrid[-12:16:.1, -15:12:.1]\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "probs = clf.predict(grid).reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour = plt.contourf(xx, yy, probs, 25, alpha = 0.6,\n",
    "                      vmin=0, vmax=1)\n",
    "ax_c = plt.colorbar(contour)\n",
    "ax_c.set_label(\"$P(y = 1)$\")\n",
    "ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],  c = [colors[int(j)] for j in y], alpha = 0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erdős Institute Data Science Boot Camp by Steven Gubkin.\n",
    "\n",
    "Please refer to the license in this repo for information on redistribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
