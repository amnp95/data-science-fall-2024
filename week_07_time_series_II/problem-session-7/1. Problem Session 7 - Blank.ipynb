{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcf6f50b",
   "metadata": {},
   "source": [
    "# Problem Session 7\n",
    "## Forecasting The Bachelorette and Pumpkin Spice II\n",
    "\n",
    "In the second of two time series based problem sessions you build upon your work in `Problem Session 6`. In particular you will look to build the best forecast you can for the Bachelorette IMDB ratings. Afterwards you will be introduced to seasonal ARIMA models with the pumpkin spice Google trends data.\n",
    "\n",
    "The problems in this notebook will cover the content covered in our `Time Series Forecasting` lectures including:\n",
    "- `Averaging and Smoothing`,\n",
    "- `Stationarity and Autocorrelation` and\n",
    "- `ARIMA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf50e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "from datetime import datetime\n",
    "\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bba8849",
   "metadata": {},
   "source": [
    "#### 1. The Bachelorette\n",
    "\n",
    "##### a.\n",
    "\n",
    "- Reload the Bachelorette IMDB data stored in `the_bachelorette.csv` in the `data` folder. \n",
    "- Look at the first five rows.\n",
    "- Then make a train test split setting aside the last three episodes as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7447489",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54347f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f04d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "tv_train = \n",
    "tv_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8721870c",
   "metadata": {},
   "source": [
    "##### b.\n",
    "\n",
    "Here is a refresher on the columns of this data.\n",
    "\n",
    "- `episode_number` is the number of the episode with respect to the entire series run,\n",
    "- `title` is the title of the episode,\n",
    "- `season` is the number of the season in which the episode aired,\n",
    "- `season_episode_number` is the number of the episode with respect to the season in which it aired,\n",
    "- `imdb_rating` is the average rating of the episode among IMDB's users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b0e41",
   "metadata": {},
   "source": [
    "##### c.\n",
    "\n",
    "The first model you will fit is a moving average model. In this problem you will be tuning the moving average window size, $q$, to find the value that minimizes the average cross-validation root mean squared error (RMSE).\n",
    "\n",
    "Fill in the missing chunks of code to perform hyperparameter tuning for $q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bf8168",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import what you need\n",
    "from sklearn.model_selection import \n",
    "from sklearn.metrics import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22271dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the Time Series CV here\n",
    "cv = \n",
    "\n",
    "## You will test from q=2 to q=30\n",
    "start = 2\n",
    "end = 31\n",
    "\n",
    "## This will hold the cv rmses\n",
    "ma_rmses = np.zeros((10, len(range(start, end))))\n",
    "\n",
    "\n",
    "i = 0\n",
    "## Loop through the cv splits\n",
    "for train_index, test_index in :\n",
    "    tv_tt = tv_train.iloc[train_index]\n",
    "    tv_ho = tv_train.iloc[test_index]\n",
    "    \n",
    "    j = 0\n",
    "    for q in range(start, end):\n",
    "        ## Get the prediction on the holdout set\n",
    "        \n",
    "        \n",
    "        ## Record the rmse on the holdout set\n",
    "        ma_rmses[i,j] = np.sqrt(mean_squared_error())\n",
    "        j = j + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c628f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will plot the average cv rmses as a function of q\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.scatter(range(start,end), np.mean(ma_rmses, axis=0))\n",
    "\n",
    "plt.xlabel(\"Window Size\", fontsize=12)\n",
    "plt.ylabel(\"Average CV RMSE\", fontsize=12)\n",
    "\n",
    "plt.xticks(range(start, end, 3), fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2d9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Which value of q had the lowest mean cv rmse?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513947e5",
   "metadata": {},
   "source": [
    "##### d.\n",
    "\n",
    "The second model you will try is an exponential smoothing model.\n",
    "\n",
    "Because these data exhibit a trend but not seasonality we will fit a double exponential smoothing model. For this we will want to find the best $\\alpha$ (The smoothing on the time series) and $\\beta$ (the smoothing on the trend component).\n",
    "\n",
    "Fill in the missing code chunks below to perform a grid search for the values of $\\alpha$ and $\\beta$ that minimize the average CV RMSE. (Note that a grid search is what we call it when you perform hyperparameter tuning with a grid of possible hyperparameter values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1431430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Holt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8761d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the grid points for alpha and beta\n",
    "## They should both run from 0 to .21 in intervals of size .01\n",
    "alphas = \n",
    "betas = \n",
    "\n",
    "## This will hold the cv rmse\n",
    "exp_rmses = np.zeros((10, len(alphas), len(betas)))\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in cv.split(tv_train):\n",
    "    tv_tt = tv_train.iloc[train_index]\n",
    "    tv_ho = tv_train.iloc[test_index]\n",
    "    \n",
    "    ## Loop through alphas and betas to fit the model and record the \n",
    "    ## holdout error\n",
    "    j = 0\n",
    "    for alpha in :\n",
    "        k = 0\n",
    "        for beta in :\n",
    "            ## This will help you keep track of the progress\n",
    "            print(\"alpha =\", alpha,\n",
    "                     \"beta =\", beta)\n",
    "\n",
    "            ## Fit the model here\n",
    "            \n",
    "            \n",
    "            exp_rmses[i,j,k] = \n",
    "            k = k + 1\n",
    "        j = j + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ff6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This gives us the indices of the smallest\n",
    "## avg cv rmse\n",
    "exp_ind = np.unravel_index(np.argmin(np.mean(exp_rmses, axis=0), axis=None), \n",
    "                           np.mean(exp_rmses, axis=0).shape)\n",
    "np.unravel_index(np.argmin(np.mean(exp_rmses, axis=0), axis=None), \n",
    "                 np.mean(exp_rmses, axis=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The alpha and beta values that give a double exponential\",\n",
    "         \"smoothing model with lowest avg cv rmse are\",\n",
    "         \"alpha = \", np.arange(0, 0.2, .01)[exp_ind[0]],\n",
    "         \"and beta = \", np.arange(0, 0.2, .01)[exp_ind[1]])\n",
    "\n",
    "print(\"This model had an avg cv rmse of\",\n",
    "         np.round(np.mean(exp_rmses, axis=0)[exp_ind],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3460aa8e",
   "metadata": {},
   "source": [
    "##### e.\n",
    "\n",
    "The final model you will try is an ARIMA model. \n",
    "\n",
    "First let's check the stationarity assumption for this time series. Make an autocorrelation plot of the training data. If you find that the ACF plot indicates that the time series is non-stationary, plot the ACF of the time series' first differences. Do these appear to be stationary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30094ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf6854",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7,5))\n",
    "\n",
    "## Make the acf plot here\n",
    "## use at least 30 lags\n",
    "\n",
    "\n",
    "\n",
    "plt.title('The Bachelorette IMDB rating ACF', fontsize=14)\n",
    "plt.ylabel(\"Autocorrelation\", fontsize=12)\n",
    "plt.xlabel(\"Lag\", fontsize=12)\n",
    "\n",
    "plt.ylim(-1.1,1.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25154d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8,5))\n",
    "\n",
    "## plot the acf for the first differences\n",
    "\n",
    "\n",
    "\n",
    "plt.title('The Bachelorette First Differences ACF', fontsize=14)\n",
    "plt.ylabel(\"Autocorrelation\", fontsize=12)\n",
    "plt.xlabel(\"Lag\", fontsize=12)\n",
    "\n",
    "plt.ylim(-1.1,1.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ae751",
   "metadata": {},
   "source": [
    "##### f.\n",
    "\n",
    "From what we saw above what we should set our $d$ value in the ARIMA model?. Set $d$ to this value and then perform hyperparameter tuning to find the values of $p$ and $q$ that give us the lowest mean CV RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62854f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import SARIMAX\n",
    "from statsmodels.tsa.api "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d0404",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up values for p and q, both should at least range from 0 to 4\n",
    "ps = \n",
    "qs = \n",
    "\n",
    "arima_rmses = np.zeros((10, 4, 4))\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in cv.split(tv_train):\n",
    "    tv_tt = tv_train.iloc[train_index]\n",
    "    tv_ho = tv_train.iloc[test_index]\n",
    "    \n",
    "    j = 0\n",
    "    for p in ps:\n",
    "        k = 0\n",
    "        for q in q:\n",
    "            ## Fit the SARIMAX model\n",
    "            arima = \n",
    "            \n",
    "            ## Get the error on the holdout set\n",
    "            arima_rmses[i,j,k] = \n",
    "            k = k +1\n",
    "        j = j + 1\n",
    "    i = i +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051d7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_ind = np.unravel_index(np.argmin(np.mean(arima_rmses, axis=0), axis=None), \n",
    "                             np.mean(arima_rmses, axis=0).shape)\n",
    "np.unravel_index(np.argmin(np.mean(arima_rmses, axis=0), axis=None), \n",
    "                 np.mean(arima_rmses, axis=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The p and q values that give an ARIMA model\",\n",
    "         \"with lowest avg cv mse are\",\n",
    "         \"p = \", range(4)[arima_ind[0]],\n",
    "         \"and q = \", range(4)[arima_ind[1]])\n",
    "\n",
    "print(\"This model had an avg cv mse of\",\n",
    "         np.round(np.mean(arima_rmses, axis=0)[arima_ind],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6081c1e",
   "metadata": {},
   "source": [
    "##### g.\n",
    "\n",
    "Compare the best RMSE you attained in this notebook to the best RMSE for the baseline models in the completed version of `Problem Session 6`.\n",
    "\n",
    "Plot the best forecast with the training and test data. What is the RMSE of the forecast on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0859b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f620c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e194f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32456751",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.scatter(tv_train.episode_number,\n",
    "               tv_train.imdb_rating,\n",
    "               alpha=.5,\n",
    "               label=\"Training Points\")\n",
    "\n",
    "plt.scatter(tv_test.episode_number,\n",
    "               tv_test.imdb_rating,\n",
    "               alpha=.5,\n",
    "               c = 'red',\n",
    "               marker = 'v',\n",
    "               label=\"Test Points\")\n",
    "\n",
    "## Input the missing code to plot your forecast\n",
    "plt.plot(tv_train.episode_number,\n",
    "            ,\n",
    "            'k-',\n",
    "            linewidth = 2,\n",
    "            label=\"Fitted Values\")\n",
    "\n",
    "plt.plot(tv_test.episode_number,\n",
    "            arima.forecast(len(tv_test)),\n",
    "            'r--',\n",
    "            linewidth=2,\n",
    "            label=\"Forecast\")\n",
    "\n",
    "plt.legend(fontsize=12, loc=4)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Episode Number\", fontsize=12)\n",
    "plt.ylabel(\"IMDB Rating\", fontsize=12)\n",
    "\n",
    "\n",
    "plt.ylim(3,8.5)\n",
    "plt.xlim(180,220)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc55f33",
   "metadata": {},
   "source": [
    "#### 2. Pumpkin spice seasonal ARIMA\n",
    "\n",
    "In this problem you will be introduced to seasonal ARIMA models with the pumpkin spice Google trend data. This will be a surface level introduction, for a more in depth look check out the time series practice problems `jupyter notebook`.\n",
    "\n",
    "##### a.\n",
    "\n",
    "Load the data stored in `pumpkin_spice.csv` in the `data` folder then look at the first five rows. Then make a train test split setting aside all observations on or after January 1, 2022 aside as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d3bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pumpkin = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d93d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train = \n",
    "p_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0ac66",
   "metadata": {},
   "source": [
    "##### b.\n",
    "\n",
    "In lecture we talked about first differencing non-stationary time series exhibiting a trend to create a, seemingly, stationary time series.\n",
    "\n",
    "This can also be done for seasonal data. Suppose that we suspect a time series, $\\left\\lbrace y_t \\right\\rbrace$ exhibits seasonality where a season lasts $m$ time steps. Then the first seasonal differenced time series is:\n",
    "\n",
    "$$\n",
    "\\nabla_s y_t = y_t - y_{t-m}.\n",
    "$$\n",
    "\n",
    "Plot the autocorrelation of the training set, then perform first seasonal differencing on these data and plot the autocorrelation of the first seasonal differenced series.\n",
    "\n",
    "Does the differenced series appear less likely to violate stationarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cfcfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here if needed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a928b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(12,5))\n",
    "\n",
    "## add the acf plot here\n",
    "\n",
    "\n",
    "plt.ylim([-1.1,1.1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e909831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(12,5))\n",
    "\n",
    "## plot the acf of the first differenced time series here\n",
    "\n",
    "\n",
    "\n",
    "plt.ylim([-1.1,1.1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493b0de6",
   "metadata": {},
   "source": [
    "Do you think you should perform second differencing or is first differencing enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code or write here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451557f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code or write here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fad46e7",
   "metadata": {},
   "source": [
    "##### c.\n",
    "\n",
    "While traditional $\\text{ARIMA}$ models do not work well for seasonal data, there are seasonal ARIMA ($\\text{SARIMA}$) models as well. Recall for an $\\text{ARIMA}$ model you needed parameters $p$, $d$ and $q$. For a $\\text{SARIMA}$ model you need parameters $P$, $D$, $Q$ and $m$ here:\n",
    "\n",
    "- $P$ is the order of the seasonal autoregressive portion of the model,\n",
    "- $Q$ is the order of the seasonal moving average portion of the model,\n",
    "- $D$ is the order of the seasonal differencing and\n",
    "- $m$ is the number of time steps that take place in a single period.\n",
    "\n",
    "You should have an idea of a value for $D$ from <i>b.</i> and we know $m=12$. In this problem you will fit a $\\text{SARIMA}$ model on the pumpkin spice data using `statsmodels` `SARIMAX`. Choose whatever values you would like for $p$, $P$, $q$ and $Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7049e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "plt.plot(p_train.Month,\n",
    "            p_train.interest_level,\n",
    "            'b-o',\n",
    "            label='Training Data')\n",
    "\n",
    "## Fill in the missing code to plot your fitted values\n",
    "plt.plot(p_train.Month,\n",
    "            sarima.fittedvalues,\n",
    "            'r',\n",
    "            label='Fitted Values')\n",
    "\n",
    "## Fill in the missing code to plot your forecast\n",
    "plt.scatter(p_test.Month,\n",
    "               ,\n",
    "               c='r',\n",
    "               marker='x',\n",
    "               s=100,\n",
    "               label=\"Forecast\")\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Date\", fontsize=14)\n",
    "plt.ylabel(\"Interest Level\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f955b77",
   "metadata": {},
   "source": [
    "##### d. \n",
    "\n",
    "Get the average cross-validation MSE for the SARIMA model you fit above. Use 5-fold cross-validation with a test set size of 12.\n",
    "\n",
    "\n",
    "How does it compare to the baseline models from `Problem Session 6`? <i>Feel free to use the answer from the completed version `Problem Session 6` if you did not complete it</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ea813",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the time series CV here\n",
    "cv = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eab0cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmses = np.zeros(5)\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in cv.split(p_train):\n",
    "    p_tt = p_train.iloc[train_index]\n",
    "    p_ho = p_train.iloc[test_index]\n",
    "    \n",
    "    ## Fit the model\n",
    "    sarima = \n",
    "    \n",
    "    rmses[i] = np.sqrt(mean_squared_error(p_ho.interest_level.values,\n",
    "                                             sarima.forecast(12)))\n",
    "    \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14380bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rmses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641e1615",
   "metadata": {},
   "source": [
    "##### Write here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168c713f",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac1662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
