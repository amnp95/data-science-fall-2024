{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelyhood Estimation (MLE) and Maximum A Posteriori (MAP) estimation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup to MLE and MAP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will introduce the ideas of MLE and MAP in a simple discrete case.\n",
    "\n",
    "#### MLE\n",
    "\n",
    "You are given a coin and you are told that it is weighted to come up heads with probability $\\theta$.  You toss the coin $3$ times and obtain the sequence HHT.  Based only on this information, what is your best estimate for $\\theta$?\n",
    "\n",
    "One way to address this rigorously is to rephrase the question this way:  of all coins with weightings $\\theta \\in [0,1]$, which value of $p$ gives the **maximum likelyhood** of producing the sequence HHT?  First make a guess about what is reasonable!\n",
    "\n",
    "The likelyhood of a coin weighted $\\theta$ producing HHT is \n",
    "\n",
    "$$L(\\theta) = \\theta^2(1-\\theta)$$\n",
    "\n",
    "The maximum value of this function over $[0,1]$ can be found using differential calculus:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\textrm{d} L}{\\textrm{d} \\theta} \n",
    "&= 2\\theta(1-\\theta) - \\theta^2\\\\\n",
    "&=2\\theta - 2\\theta^2 - \\theta^2\\\\\n",
    "&=2\\theta - 3\\theta^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So the derivative is $0$ when $\\theta = 0$ or $\\theta = \\frac{2}{3}$.  As $L(0) = L(1) = 0$ and $L(\\frac{2}{3}) > 0$ we have that the global maximum of $L$ occurs at $\\theta = \\frac{2}{3}$.  \n",
    "\n",
    "Does this agree with the guess you made before doing the Calculus?\n",
    "\n",
    "Let's do the same thing, but imagine that you tossed the coin $n$ times, got $m$ heads and $n-m$ tails.\n",
    "\n",
    "The probability of a coin weighted $\\theta$ producing $m$ heads and $n-m$ tails.\n",
    "\n",
    "$$L(\\theta) = \\theta^n(1-\\theta)^{n-m}$$\n",
    "\n",
    "We could differentiate this directly, but we will get a cleaner result if we use logarithmic differentiation.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log(L)(\\theta) &= \\log(\\theta^m(1-\\theta)^{n-m})\\\\\n",
    "&= m\\log(\\theta) + (n-m)\\log(1-\\theta)\\\\\n",
    "\\frac{\\textrm{d}}{\\textrm{d}\\theta} \\log(L(\\theta)) = \\frac{m}{\\theta} - \\frac{n-m}{1-\\theta}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Setting this equal to zero we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{m}{\\theta} - \\frac{n-m}{1-\\theta} &= 0\\\\\n",
    "m(1-\\theta) - (n-m)\\theta &= 0\\\\\n",
    "m - m\\theta - n\\theta + m\\theta &= 0\\\\\n",
    "\\theta &= \\frac{m}{n} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "which should also agree with your intuition!  \n",
    "\n",
    "#### MAP\n",
    "\n",
    "Real world coins have $\\theta \\approx \\frac{1}{2}$.  In the situation above we are **told** that the coin (might be) biased, and so it is reasonable to guess that $\\theta = \\frac{4}{5}$ if you observe $4$ heads and $1$ tail.  Without being given any such information, we might instead think that the distribution of weights could look something like this:\n",
    "\n",
    "<p align=\"center\">\n",
    "\n",
    "<img src=\"math_hour_1_assets/beta_prior.png\" alt=\"beta prior\" style=\"width:200px;\"/>\n",
    "\n",
    "</p>\n",
    "\n",
    "This is an example of a \"prior distribution\" on the parameter $\\theta$.  This particular prior is \n",
    "\n",
    "$$\n",
    "k \\theta^8 (1-\\theta)^8\n",
    "$$\n",
    "\n",
    "where the constant $k$ has been chosen to make the integral equal to $1$ over $[0,1]$ (it turns out that $k = \\frac{1}{B(9,9)}$ where $B$ is the \"beta\" function. See [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) for more info).\n",
    "\n",
    "Assume that we flip our coin $5$ times and obtain $4$ heads as before. Instead of estimating $\\theta = \\frac{m}{n}$ we should now guess something in between $\\frac{4}{5}$ and $\\frac{1}{2}$.  How can we calculate it exactly?  Bayes' Rule to the rescue!\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(\\theta | \\textrm{Data}) \n",
    "&\\propto P(\\textrm{Data}| \\theta) \\cdot P(\\theta)\\\\\n",
    "&\\propto \\theta^4(1-\\theta)^1 \\cdot \\theta^8(1-\\theta)^8\\\\\n",
    "&\\propto \\theta^{4+8}(1-\\theta)^{1+8}\\\\\n",
    "&= \\theta^{12} (1 - \\theta)^9\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We have maximized this kind of thing before:  the maximum value occurs at $\\hat{\\theta} = \\frac{12}{21}$.\n",
    "\n",
    "MAP (in this case) was equivalent to MLE with 8 extra heads and tails than what we actually observed.  This is a nice interpretation:  it is one way of making the statement that \"most coins have $\\theta \\approx \\frac{1}{2}$\" precise.\n",
    "\n",
    "In general MLE is equivalent to MAP with a uniform prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE estimates of mean and variance\n",
    "\n",
    "We now leave the discrete world.\n",
    "\n",
    "Assume that $n$ samples $x_1, x_2, x_3, \\dots , x_n$ are drawn from a normal distribution with (unknown) mean $\\mu$ and (unknown) variance $\\sigma^2$.\n",
    "\n",
    "We want to find the maximum likelyhood estimates $\\hat{\\mu}$ and $\\hat{\\sigma}^2$ of these parameters.  You might be able to guess what these are!  Absent any other informationl, what would your best guess for $\\mu$ and $\\sigma$ be?\n",
    "\n",
    "In other words, we want to maximize\n",
    "\n",
    "$$\n",
    "\\operatorname{L}(\\mu, \\sigma) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left(-\\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "Again, it is more convenient to turn this product into a sum using the logarithm. This is the **log likelyhood**.\n",
    "\n",
    "$$\n",
    "\\operatorname{LL}(\\mu, \\sigma) = \\log\\left( \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\left(-\\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right) \\right)\n",
    "$$\n",
    "\n",
    "After a little algebra we have \n",
    "\n",
    "$$\n",
    "\\operatorname{LL}(\\mu, \\sigma)  = \\left[ -n \\log{\\sqrt{2\\pi}} - n\\log(\\sigma) - \\sum_{i=1}^{n} \\frac{(x_i-\\mu)^2}{2\\sigma^2} \\right]\n",
    "$$\n",
    "\n",
    "This is a lot of minus signs, so we can instead think about minimizing the **negative log likelyhood**\n",
    "\n",
    "$$\n",
    "\\operatorname{NLL}(\\mu, \\sigma) = \\left[ n \\log{\\sqrt{2\\pi}} + n\\log(\\sigma) + \\sum_{i=1}^{n} \\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right]\n",
    "$$\n",
    "\n",
    "I will switch to using $\\ell$ in place of $\\operatorname{NLL}$ to make it look pretty.\n",
    "\n",
    "We can now take partial derivatives\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\ell}{\\partial \\mu} &= \\sum_{i=1}^{n} \\frac{(x_i-\\mu)}{\\sigma^2}\\\\\n",
    "\\frac{\\partial \\ell}{\\partial \\mu} &= \\frac{n}{\\sigma} - \\sum_{i=1}^{n} \\frac{(x_i-\\mu)^2}{\\sigma^3}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Setting both equal to $0$ and solving we obtain\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\mu} &= \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\bar{x}\\\\\n",
    "\\hat{\\sigma}^2 &= \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go from here?\n",
    "\n",
    "We will use MLE constantly in math hour!  In particular, MLE comes up in:\n",
    "\n",
    "* Linear Regression\n",
    "* Logistic Regression\n",
    "* Gaussian Discriminant Analysis\n",
    "\n",
    "More generally, MLE follows a common machine learning pattern which we will see again and again:\n",
    "\n",
    "* We have some data.\n",
    "* We create a model for a data generating process which depends on some parameters.\n",
    "* We specify a \"loss function\": how do we judge \"how well\" a given set of parameters models the data?\n",
    "* We minimize the loss function, often by using gradient descent when exact solutions are unavailable."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
