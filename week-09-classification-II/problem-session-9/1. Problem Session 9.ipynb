{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b382d20a",
   "metadata": {},
   "source": [
    "# Problem Session 9\n",
    "## Classifying Pumpkin Seeds II\n",
    "\n",
    "In this notebook you continue to work with the pumpkin seed data from <a href=\"https://link.springer.com/article/10.1007/s10722-021-01226-0\">The use of machine learning methods in classification of pumpkin seeds (Cucurbita pepo L.)</a> by Koklu, Sarigil and Ozbek (2021).\n",
    "\n",
    "The problems in this notebook will cover the content covered in some of our `Classification` notebooks as well as some of our `Dimension Reduction` notebooks. In particular we will cover content touched on in:\n",
    "- `Classification/Adjustments for Classification`,\n",
    "- `Classification/k Nearest Neighbors`,\n",
    "- `Classification/The Confusion Matrix`,\n",
    "- `Classification/Logistic Regression`,\n",
    "- `Classification/Diagnostic Curves`,\n",
    "- `Classification/Bayes Based Classifiers` and\n",
    "- `Dimension Reduction/Principal Components Analysis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b9f02d",
   "metadata": {},
   "source": [
    "#### 1. Load then prepare the data\n",
    "\n",
    "\n",
    "- Load the data stored in `Pumpkin_Seeds_Dataset.xlsx` in the `Data` folder,\n",
    "- Create a column `y` where `y=1` if `Class=Ürgüp Sivrisi` and `y=0` if `Class=Çerçevelik` and\n",
    "- Make a train test split setting $10\\%$ of the data aside as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36da466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = pd.read_excel(\"../Data/Pumpkin_Seeds_Dataset.xlsx\")\n",
    "\n",
    "## Create the y column here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8fd445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2920249",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the train test split here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bee9ccf",
   "metadata": {},
   "source": [
    "#### 2. Refresh your memory\n",
    "\n",
    "If you need to refresh your memory on these data and the problem, you may want to look at a small subset of the data, look back on `Problem Session 8` and/or browse Figure 5 and Table 1 of this paper, <a href=\"pumpkin_seed_paper.pdf\">pumpkin_seed_paper.pdf</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a26c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e8a15e",
   "metadata": {},
   "source": [
    "#### 3. Principal components analysis (PCA)\n",
    "\n",
    "One way you may use PCA is as a data preprocessing step for supervised learning tasks. In this problem you will try it as a preprocessing step for the pumpkin seed data and see if this preprocessing step helps your model outperform the models from `Problem Session 8`.\n",
    "\n",
    "##### a. \n",
    "\n",
    "Run the training data through PCA with two components and then plot the resulting principal values. Color each point by its class.\n",
    "\n",
    "<i>Hint: Remember to scale the data before running it through PCA</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11842e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import what you need for PCA\n",
    "from sklearn.decomposition import\n",
    "from sklearn.preprocessing import\n",
    "from sklearn.pipeline import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac1fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the PCA Pipeline here\n",
    "\n",
    "\n",
    "## Fit the PCA here\n",
    "\n",
    "\n",
    "## Get the transformed training data here\n",
    "## Store it in the variable fit provided below\n",
    "fit = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950afa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code will help you plot the two classes\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "## Plot y=0 class here\n",
    "plt.scatter(fit[, 0], \n",
    "            fit[, 1],\n",
    "            color = 'b',\n",
    "            label=,\n",
    "            alpha=.6)\n",
    "\n",
    "## Plot the y=1 class here\n",
    "plt.scatter(fit[, 0], \n",
    "            fit[, 1],\n",
    "            color='r',\n",
    "            marker='x',\n",
    "            label=,\n",
    "            alpha=.6)\n",
    "\n",
    "plt.legend(fontsize=10)\n",
    "\n",
    "plt.xlabel(\"First PCA Value\", fontsize=12)\n",
    "plt.ylabel(\"Second PCA Value\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158c4c8b",
   "metadata": {},
   "source": [
    "##### b.\n",
    "\n",
    "How does the PCA with only two componenets appear to separate the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91b2c3e",
   "metadata": {},
   "source": [
    "##### Write here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d8fabf",
   "metadata": {},
   "source": [
    "##### c.\n",
    "\n",
    "Run 5-fold cross-validation below to find the optimal value of $k$ for a $k$ nearest neighbors model fit on the first and second PCA values. What is the optimal $k$ and the associated average cross-validation accuracy? How does this compare to the accuracies from `Problem Session 8`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e774de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import what you need here\n",
    "from sklearn.neighbors import\n",
    "from sklearn.model_selection import\n",
    "from sklearn.metrics import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c88813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the kfold object here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f63f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill in the missing code below to run the cross validation\n",
    "ks = range(1, 51)\n",
    "\n",
    "## Make an array to hold the accuracies\n",
    "pca_2_accs = np.zeros()\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in kfold.split(seeds_train, seeds_train.y):\n",
    "    ## This will help you keep track of the loop progress\n",
    "    print(\"CV Split\", i)\n",
    "    seeds_tt = seeds_train.iloc[train_index]\n",
    "    seeds_ho = seeds_train.iloc[test_index]\n",
    "    \n",
    "    \n",
    "    ## Note, putting the PCA here speeds up the loop\n",
    "    ## Make the PCA pipeline\n",
    "    pca_pipe = \n",
    "    \n",
    "    ## Fit and then get the PCA transformed tt data here\n",
    "    pca_tt = \n",
    "    \n",
    "    ## Get the transformed holdout data here\n",
    "    pca_ho = pca_pipe.transform(seeds_ho[features].values)\n",
    "    \n",
    "    j = 0\n",
    "    for k in ks:\n",
    "        ## Define the KNN model here\n",
    "        knn = \n",
    "        \n",
    "        ## Fit the knn model\n",
    "        knn\n",
    "        \n",
    "        ## Get the prediction on the holdout data\n",
    "        pred = \n",
    "        \n",
    "        ## Records the accuracy\n",
    "        pca_2_accs[i,j] = accuracy_score(seeds_ho.y.values, pred)\n",
    "        \n",
    "        j = j + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502715b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Will plot the avg cv accuracy as a function of k\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "\n",
    "plt.plot(ks, \n",
    "         np.mean(pca_2_accs, axis=0),\n",
    "         '-o')\n",
    "\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.xlabel(\"$k$\", fontsize=12)\n",
    "plt.ylabel(\"Avg. CV Accuracy\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e666c6a",
   "metadata": {},
   "source": [
    "##### d.\n",
    "\n",
    "We can think of the number of components used in PCA as another hyperparameter we can tune.\n",
    "\n",
    "Fill in the missing code below to find the optimal number of components and $k$ pairing for this problem. What is the best average cross-validation accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a6b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 51)\n",
    "comps = range(2,6)\n",
    "\n",
    "## Makes a 3-D array of to record the accuracies in\n",
    "pca_accs = np.zeros((n_splits, len(comps), len(ks)))\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in kfold.split(seeds_train, seeds_train.y):\n",
    "    ## Keeps track of the cross validation split you are on\n",
    "    ## This loop can be a little long\n",
    "    print(\"CV Split\", i)\n",
    "    seeds_tt = seeds_train.iloc[train_index]\n",
    "    seeds_ho = seeds_train.iloc[test_index]\n",
    "    \n",
    "    j = 0\n",
    "    for n_comps in comps:\n",
    "        ## Make the PCA pipeline here\n",
    "        pca_pipe = \n",
    "    \n",
    "        ## Fit the PCA pipeline and get the transformed tt data\n",
    "        pca_tt = \n",
    "        \n",
    "        ## Get the transformed holdout data here\n",
    "        pca_ho = \n",
    "        \n",
    "        k = 0\n",
    "        for neighbors in ks:\n",
    "            knn = KNeighborsClassifier(neighbors)\n",
    "            \n",
    "            ## Fit on the tt data\n",
    "            knn.fit()\n",
    "\n",
    "            ## Get the holdout prediction\n",
    "            pred = knn.predict()\n",
    "\n",
    "            pca_accs[i,j,k] = accuracy_score(seeds_ho.y.values, pred)\n",
    "            \n",
    "            k = k + 1\n",
    "        j = j + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d139d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code will print out the best # components - k combo for you\n",
    "## It also prints out the highest AVG CV Accuracy\n",
    "max_index = np.unravel_index(np.argmax(np.mean(pca_accs, axis=0), axis=None), \n",
    "                                       np.mean(pca_accs, axis=0).shape)\n",
    "\n",
    "\n",
    "print(\"The pair with the highest AVG CV Accuracy was\",\n",
    "         \"k =\", ks[max_index[1]],\n",
    "         \"and number of components =\", np.round(comps[max_index[0]],2))\n",
    "print(\"The highest AVG CV Accuracy was\", np.max(np.mean(pca_accs, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09293b3",
   "metadata": {},
   "source": [
    "#### 4. Trying Bayes based classifiers\n",
    "\n",
    "Build LDA, QDA and naive Bayes' models on these data by filling in the missing code for the cross-validation below. \n",
    "\n",
    "Do these outperform your PCA-$k$NN model from above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60674ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import what you need\n",
    "from sklearn.naive_bayes import \n",
    "from sklearn.discriminant_analysis import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba1b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a holder for the accuracies\n",
    "bayes_accs = np.zeros()\n",
    "\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in kfold.split(seeds_train, seeds_train.y):\n",
    "    seeds_tt = seeds_train.iloc[train_index]\n",
    "    seeds_ho = seeds_train.iloc[test_index]\n",
    "    \n",
    "    ## Linear Discriminant Analysis\n",
    "    lda = \n",
    "    \n",
    "    lda.fit()\n",
    "    lda_pred = lda.predict()\n",
    "    \n",
    "    ## Records the accuracies\n",
    "    bayes_accs[i, 0] = accuracy_score(seeds_ho.y.values,\n",
    "                                         lda_pred)\n",
    "    \n",
    "    ## Quadratic Discriminant Analysis\n",
    "    qda = \n",
    "    \n",
    "    qda.fit()\n",
    "    \n",
    "    qda_pred = qda.predict()\n",
    "    \n",
    "    ## Records the accuracies\n",
    "    bayes_accs[i, 1] = accuracy_score(seeds_ho.y.values,\n",
    "                                         qda_pred)\n",
    "    \n",
    "    \n",
    "    ## Gaussian Naive Bayes\n",
    "    nb = \n",
    "    \n",
    "    nb.fit()\n",
    "    nb_pred = nb.predict()\n",
    "    \n",
    "    ## Records the accuracies\n",
    "    bayes_accs[i, 2] = accuracy_score(seeds_ho.y.values,\n",
    "                                         nb_pred)\n",
    "    \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2755d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print the Average CV Accuracies here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059eadc8",
   "metadata": {},
   "source": [
    "#### 5. LDA for supervised dimensionality reduction\n",
    "\n",
    "While we introduced linear discriminant analysis (LDA) as a classification algorithm, it was originally proposed by Fisher as a supervised dimension reduction technique, <a href=\"https://digital.library.adelaide.edu.au/dspace/bitstream/2440/15227/1/138.pdf\">https://digital.library.adelaide.edu.au/dspace/bitstream/2440/15227/1/138.pdf</a>. In particular, the initial goal was to project the features, $X$, corresponding to a binary output, $y$, onto a single dimension which best separates the possible classes. This single dimension has come to been known as <i>Fisher's discriminant</i>.\n",
    "\n",
    "Walk through the code below to perform this supervised dimension reduction technique on these data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b17e3c",
   "metadata": {},
   "source": [
    "##### a.\n",
    "\n",
    "First make a validation set from the training set for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc74fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we make a validation set for demonstration purposes\n",
    "seed_tt, seeds_val = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d912df47",
   "metadata": {},
   "source": [
    "##### b.\n",
    "\n",
    "Now make a pipeline that first scales the data and ends with linear discriminant analysis. Then fit the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d30ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a76d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the pipeline here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1034609",
   "metadata": {},
   "source": [
    "##### c. \n",
    "\n",
    "Now calculate the Fisher discriminant by using `transform` with the pipeline you fit in <i>b.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f86ffdc",
   "metadata": {},
   "source": [
    "##### d. \n",
    "\n",
    "To visualize how LDA separated the two classes while projecting the 12 dimensional data onto a one dimensional subspace you can plot a histogram of the Fisher discriminant colored by the pumpkin seed class of the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcf457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "## Fill in the code below\n",
    "## Plot the fisher's discriminant for y=0\n",
    "plt.hist(, \n",
    "         color='blue',\n",
    "         edgecolor=\"black\",\n",
    "         label=\"$y=0$\")\n",
    "\n",
    "## Plot the fisher's discriminant for y=1\n",
    "plt.hist(, \n",
    "         color='orange', \n",
    "         hatch='/', \n",
    "         alpha=.6,\n",
    "         edgecolor=\"black\",\n",
    "         label=\"$y=1$\")\n",
    "\n",
    "plt.xlabel(\"Fisher Discriminant\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a9eccb",
   "metadata": {},
   "source": [
    "##### e.\n",
    "\n",
    "While there is some separation between the two classes, it is not perfect, this should be expected based on the exploratory data analysis you did in `Problem Session 8`.\n",
    "\n",
    "We could use this discriminant in order to make classifications, for example by setting a simple cutoff value or as input into a different classification algorithm.\n",
    "\n",
    "However, it is important to note that the LDA algorithm maximizes the separation of the two classes among observations of the training set. It is possible that separation would not be as good for data the algorithm was not trained on.\n",
    "\n",
    "In this example we can visually inspect by plotting a histogram of the Fisher discriminant values for the validation set we created. Does the separation seem as pronounced on the validation data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the discriminant for the validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ee620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "## Fill in the missing code below\n",
    "\n",
    "## Plot the validation discriminant for the y=0 observations\n",
    "plt.hist(, \n",
    "         color='blue',\n",
    "         edgecolor=\"black\",\n",
    "         label=\"$y=0$\")\n",
    "\n",
    "## Plot the validation discriminant for the y=1 observations\n",
    "plt.hist(, \n",
    "         color='orange', \n",
    "         hatch='/', \n",
    "         alpha=.6,\n",
    "         edgecolor=\"black\",\n",
    "         label=\"$y=1$\")\n",
    "\n",
    "plt.xlabel(\"Fisher Discriminant\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc242bfc",
   "metadata": {},
   "source": [
    "##### Write any notes you would like here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b990a",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af011233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
